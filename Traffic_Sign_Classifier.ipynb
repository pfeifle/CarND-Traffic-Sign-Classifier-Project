{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = '.\\\\data\\\\train.p'\n",
    "testing_file = '.\\\\data\\\\test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "#mp X_train, y_train = train['features'], train['labels']\n",
    "#mp X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 2D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**\n",
    "\n",
    "Complete the basic data summary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 39209\n",
      "Number of testing examples = 12630\n",
      "Image data shape (train object sizes 0) = [29 30]\n",
      "Image data shape (train object features 0) = (32, 32, 3)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42]\n",
      "Number of training classes = 43\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42]\n",
      "Number of testing classes = 43\n"
     ]
    }
   ],
   "source": [
    "### Replace each question mark with the appropriate value.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = len(train['features']) #ok\n",
    "# TODO: Number of testing examples.\n",
    "n_test = len(test['features']) #ok\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape_train = train['sizes'][0] # unclear what to do here in detail. The shape of the first image is returned\n",
    "image_shape_train_features = train['features'][0].shape # unclear what to do here in detail. The shape of the first image is returned\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes_train = len(np.unique(train['labels'])) #ok\n",
    "n_classes_test = len(np.unique(test['labels'])) #ok\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape (train object sizes 0) =\", image_shape_train)\n",
    "print(\"Image data shape (train object features 0) =\", image_shape_train_features)\n",
    "print(np.unique(train['labels']))\n",
    "print(\"Number of training classes =\", n_classes_train)\n",
    "print(np.unique(test['labels']))\n",
    "print(\"Number of testing classes =\", n_classes_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the German Traffic Signs Dataset using the pickled file(s). This is open ended, suggestions include: plotting traffic sign images, plotting the count of each sign, etc.\n",
    "\n",
    "The [Matplotlib](http://matplotlib.org/) [examples](http://matplotlib.org/examples/index.html) and [gallery](http://matplotlib.org/gallery.html) pages are a great resource for doing visualizations in Python.\n",
    "\n",
    "**NOTE:** It's recommended you start with something simple first. If you wish to do more, come back to it after you've completed the rest of the sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH4AAAB6CAYAAAB5sueeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4bfb9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(test['features']))\n",
    "image = test['features'][index]\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "#plt.imshow(image)\n",
    "print(test['labels'][index])\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block contains some \"playing code\" to understand the input data better. The project does not need this code block. The code should finally be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "[27 27]\n",
      "[ 5  5 22 22]\n",
      "x-extension:  0.744186\n",
      "y-extension:  0.767442\n",
      "12630\n",
      "12630\n",
      "39209\n",
      "39209\n",
      "[43 43]\n",
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "### some test functions to understand the input data better\n",
    "\n",
    "i=50\n",
    "print(test['labels'][i])\n",
    "print(test['sizes'][i])\n",
    "print(test['coords'][i])\n",
    "print(\"x-extension: \", (train['coords'][i][2]-train['coords'][i][0]) / train['sizes'][i][0])\n",
    "print(\"y-extension: \", (train['coords'][i][3]-train['coords'][i][1]) / train['sizes'][i][1])\n",
    "\n",
    "print(len(test['sizes']))\n",
    "print(len(test['features']))\n",
    "print(len(train['sizes']))\n",
    "print(len(train['features']))\n",
    "print(train['sizes'][i])\n",
    "print(test['features'][3].shape)\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Neural network architecture\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these.\n",
    "\n",
    "**NOTE:** The LeNet-5 implementation shown in the [classroom](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) at the end of the CNN lesson is a solid starting point. You'll have to change the number of classes and possibly the preprocessing, but aside from that it's plug and play!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH4AAAB6CAYAAAB5sueeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nT+EA8fsUDhC/T+EA8fsU/h/GbdpYPAjrvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7bda278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In this code block the training images will be transformed into \n",
    "# gray-scale images. Each image can later on be represented as a 32x32 \n",
    "# feature vector\n",
    "# Input: train['features']\n",
    "# Output: gray_train_features\n",
    "# Author: Martin Pfeifle\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "def grayscale(img): # copy from lane detection program\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray_train_features  = np.zeros((len(train['features']),32,32))\n",
    "for i in range(len(train['features'])):\n",
    "    gray_train_features[i] = grayscale (train['features'][i])\n",
    "\n",
    "#Test - Depict one gray training image\n",
    "index = random.randint(0, len(gray_train_features))\n",
    "image = gray_train_features[index]\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(train['labels'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH4AAAB6CAYAAAB5sueeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEaUZ8EeU/hcsOJETWuQvOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8bebe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In this code block the testing images will be transformed into \n",
    "# gray-scale images of size 32x32. Each image can later on be represented as a 32x32 \n",
    "# feature vector. Both normalization and grayscale is done for the test images\n",
    "# in this function.\n",
    "# Input: test['features']\n",
    "# Output: gray_test_features\n",
    "# Author: Martin Pfeifle\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "def grayscale(img): # copy from lane detection program\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray_test_features  = np.zeros((len(test['features']),32,32))\n",
    "for i in range(len(test['features'])):\n",
    "    gray_test_features[i] = grayscale (test['features'][i])\n",
    "\n",
    "#Test - Depict one gray test image\n",
    "index = random.randint(0, len(gray_test_features))\n",
    "image = gray_test_features[index]\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(test['labels'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block creates\n",
    "* X_train\n",
    "* y_train\n",
    "* X_Validation\n",
    "* y_valdiation\n",
    "* X_test\n",
    "* y_test\n",
    "\n",
    "The idea is that x_train and X_Validation contain data in a 2-dimensional matrix (number of train images, 1024). So each image is represented as a 1024 dimensional feature vector. y_train and y_validation contain the corresponding one-hot encoded class values.\n",
    "\n",
    "X_test and y_test have the same format but contain different images which are not used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train = np.empty([len(gray_train_features),1024])\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i]=np.reshape(gray_train_features[i],1024)\n",
    "y_train = train['labels']\n",
    "\n",
    "X_validation, y_validation = X_train, y_train\n",
    "\n",
    "X_test = np.empty([len(gray_test_features),1024])\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i]=np.reshape(gray_test_features[i],1024)\n",
    "y_test = test['labels']\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter *x* for LeNet is a 1024-dimensional feature vector which is first transformed into a 32x32 image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):\n",
    "    # Reshape from 2D to 4D. This prepares the data for\n",
    "    # convolutional and pooling layers.\n",
    "    x = tf.reshape(x, (-1, 32, 32, 1))\n",
    "    # Pad 0s to 32x32. Centers the digit further.\n",
    "    # Add 2 rows/columns on each side for height and width dimensions.\n",
    "    # mp: was commented out. strangely enough it was working with padding although the  input is already 32x32 \n",
    "    # x = tf.pad(x, [[0, 0], [2, 2], [2, 2], [0, 0]], mode=\"CONSTANT\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # SOLUTION: Convolution Layer 1. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation 1.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling Layer 1. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Convolution Layer 2. Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation 2.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling Layer 2. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten Layer.\n",
    "    fc1 = flatten(conv2)\n",
    "    fc1_shape = (fc1.get_shape().as_list()[-1], 120)\n",
    "    \n",
    "    # SOLUTION: Fully Connected Layer 1. Input = 5x5x16. Output = 120.\n",
    "    fc1_W     = tf.Variable(tf.truncated_normal(shape=(fc1_shape), mean = mu, stddev = sigma))\n",
    "    fc1_b     = tf.Variable(tf.zeros(120))\n",
    "    fc1       = tf.matmul(fc1, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation 3.\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Fully Connected Layer 2. Input = 120. Output = 43.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 43), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy, total_loss = 0, 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        loss, accuracy =  sess.run([loss_operation, accuracy_operation], feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * batch_x.shape[0])\n",
    "        total_loss     += (loss * batch_x.shape[0])\n",
    "    return total_loss / num_examples, total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 1024))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = LeNet(x)\n",
    "loss_operation = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y))\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Loss     = 0.997\n",
      "Validation Accuracy = 0.752\n",
      "Model saved  0\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Loss     = 0.627\n",
      "Validation Accuracy = 0.841\n",
      "Model saved  1\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Loss     = 0.391\n",
      "Validation Accuracy = 0.905\n",
      "Model saved  2\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Loss     = 0.278\n",
      "Validation Accuracy = 0.930\n",
      "Model saved  3\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Loss     = 0.258\n",
      "Validation Accuracy = 0.933\n",
      "Model saved  4\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Loss     = 0.237\n",
      "Validation Accuracy = 0.937\n",
      "Model saved  5\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Loss     = 0.144\n",
      "Validation Accuracy = 0.961\n",
      "Model saved  6\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Loss     = 0.131\n",
      "Validation Accuracy = 0.969\n",
      "Model saved  7\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Loss     = 0.150\n",
      "Validation Accuracy = 0.965\n",
      "Model saved  8\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Loss     = 0.146\n",
      "Validation Accuracy = 0.966\n",
      "Model saved  9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)    \n",
    "    try:\n",
    "        saver\n",
    "    except NameError:\n",
    "        saver = tf.train.Saver()\n",
    "    # Important so that in each Epoch run examples from all traffic sign classes are used    \n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range (EPOCHS):\n",
    "        X_validation_epoch_run = X_train[int(i*0.1*num_examples): int((i+1)*0.1*num_examples)]\n",
    "        y_validation_epoch_run = y_train[int(i*0.1*num_examples): int((i+1)*0.1*num_examples)]\n",
    "        \n",
    "        X_train_epoch_run_1 = X_train[0: int(i*0.1*num_examples)]\n",
    "        X_train_epoch_run_2 = X_train[int((i+1)*0.1*num_examples)+1:num_examples] \n",
    "        X_train_epoch_run = np.concatenate ((X_train_epoch_run_1,X_train_epoch_run_2))\n",
    "        \n",
    "        y_train_epoch_run_1 = y_train[0: int(i*0.1*num_examples)]\n",
    "        y_train_epoch_run_2 = y_train[int((i+1)*0.1*num_examples)+1:num_examples] \n",
    "        y_train_epoch_run = np.concatenate ((y_train_epoch_run_1,y_train_epoch_run_2))\n",
    "        \n",
    "        X_train_epoch_run, y_train_epoch_run = shuffle(X_train_epoch_run, y_train_epoch_run)\n",
    "        for offset in range(0, len(X_train_epoch_run), BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train_epoch_run[offset:end], y_train_epoch_run[offset:end]\n",
    "            loss = sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_loss, validation_accuracy = evaluate(X_validation_epoch_run, y_validation_epoch_run)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Loss     = {:.3f}\".format(validation_loss))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        saver.save(sess, '.\\lenet',global_step=i)\n",
    "        print(\"Model saved \", i)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss     = 1.274\n",
      "Test Accuracy = 0.867\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    loader = tf.train.import_meta_graph('lenet-9.meta')\n",
    "    loader.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    test_loss, test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Loss     = {:.3f}\".format(test_loss))\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Question 1 \n",
    "\n",
    "_Describe how you preprocessed the data. Why did you choose that technique?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* The coloured training and testing traffic sign immages were transformed into gray images by using OpenCV cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), i.e. an image is transformed from (32x32x3) to (32x32x1)\n",
    "* Botht the training and testing data were transformed into a one dimensional feature vector, i.e. a transformation from (32x32x1) to (0124x1)\n",
    "* Based on this techique the LeNet code from LeNet-Lab could be used by only  a simple change, i.e. removal of the padding step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "_Describe how you set up the training, validation and testing data for your model. **Optional**: If you generated additional data, how did you generate the data? Why did you generate the data? What are the differences in the new dataset (with generated data) from the original dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* The input data were already divided into Training nd Test data\n",
    "* The given ratio between Test/(Test+Training) was 12630 / (12630+39209) which fits quite well to the 20%-rule of thumb mentioned in the course.\n",
    "* The concept of cross-validation was applied for the training set. In each run of the 10-runs, 10% of the training data were used for validation and 90% for the training. In the first run, the validation set contained the first 10% of the training data, in the 2nd set it contained training data from 10-20%, etc.\n",
    "* Epoch was set to 10 as there seems no big accuracy improvement any more after run 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)  For reference on how to build a deep neural network using TensorFlow, see [Deep Neural Network in TensorFlow\n",
    "](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/b516a270-8600-4f93-a0a3-20dfeabe5da6/concepts/83a3a2a2-a9bd-4b7b-95b0-eb924ab14432) from the classroom._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model was exactly the sameone as used in the LetNet-Lab except that the first padding step which enlarged the images from 28x28 to 32x32 was skipped as the traffic signs were already of size 32x32. So the following steps were carried out:\n",
    "* Convolution Layer with Input = 32x32x1 and Output = 28x28x6.\n",
    "* Activation with relu\n",
    "* Pooling Layer with Input = 28x28x6 and Output = 14x14x6.\n",
    "* Convolution Layer with Output = 10x10x16\n",
    "* Activation with relu \n",
    "* Pooling Layer with Input = 10x10x16 and Output = 5x5x16.\n",
    "* Flatten Layer from 5x5x16 to 400x1 \n",
    "* Fully Connected Layer with Input =400 and Output = 120.\n",
    "* Activation with relu\n",
    "* Fully Connected Layer with Input = 120 and Output = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "_How did you train your model? (Type of optimizer, batch size, epochs, hyperparameters, etc.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The parameters were the same as in LetNet-Lab project:\n",
    "* Epoch: 10\n",
    "* Batch Size: 50\n",
    "* distribution parameters for initial weights:  mu = 0 and sigma = 0.1\n",
    "* intial bias values were set to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "_What approach did you take in coming up with a solution to this problem? It may have been a process of trial and error, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think this is suitable for the current problem._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The taken approach was to use the already existing LeNet-5 architecture model which is working well for theThe parameters were the same as in LetNet-Lab project:\n",
    "* Epoch: 10\n",
    "* Batch Size: 50\n",
    "* distribution parameters for initial weights:  mu = 0 and sigma = 0.1\n",
    "* intial bias values were set to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "Take several pictures of traffic signs that you find on the web or around you (at least five), and run them through your classifier on your computer to produce example results. The classifier might not recognize some local signs but it could prove interesting nonetheless.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "_Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult? It could be helpful to plot the images in the notebook._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run the predictions here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "_Is your model able to perform equally well on captured pictures when compared to testing on the dataset? The simplest way to do this check the accuracy of the predictions. For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate._\n",
    "\n",
    "_**NOTE:** You could check the accuracy manually by using `signnames.csv` (same directory). This file has a mapping from the class id (0-42) to the corresponding sign name. So, you could take the class id the model outputs, lookup the name in `signnames.csv` and see if it matches the sign from the image._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize the softmax probabilities here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "*Use the model's softmax probabilities to visualize the **certainty** of its predictions, [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#top_k) could prove helpful here. Which predictions is the model certain of? Uncertain? If the model was incorrect in its initial prediction, does the correct prediction appear in the top k? (k should be 5 at most)*\n",
    "\n",
    "`tf.nn.top_k` will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it'll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids.\n",
    "\n",
    "Take this numpy array as an example:\n",
    "\n",
    "```\n",
    "# (5, 6) array\n",
    "a = np.array([[ 0.24879643,  0.07032244,  0.12641572,  0.34763842,  0.07893497,\n",
    "         0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.08594638,  0.0178669 ,  0.18063401,\n",
    "         0.15899337],\n",
    "       [ 0.26076848,  0.23664738,  0.08020603,  0.07001922,  0.1134371 ,\n",
    "         0.23892179],\n",
    "       [ 0.11943333,  0.29198961,  0.02605103,  0.26234032,  0.1351348 ,\n",
    "         0.16505091],\n",
    "       [ 0.09561176,  0.34396535,  0.0643941 ,  0.16240774,  0.24206137,\n",
    "         0.09155967]])\n",
    "```\n",
    "\n",
    "Running it through `sess.run(tf.nn.top_k(tf.constant(a), k=3))` produces:\n",
    "\n",
    "```\n",
    "TopKV2(values=array([[ 0.34763842,  0.24879643,  0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.18063401],\n",
    "       [ 0.26076848,  0.23892179,  0.23664738],\n",
    "       [ 0.29198961,  0.26234032,  0.16505091],\n",
    "       [ 0.34396535,  0.24206137,  0.16240774]]), indices=array([[3, 0, 5],\n",
    "       [0, 1, 4],\n",
    "       [0, 5, 1],\n",
    "       [1, 3, 5],\n",
    "       [1, 4, 3]], dtype=int32))\n",
    "```\n",
    "\n",
    "Looking just at the first row we get `[ 0.34763842,  0.24879643,  0.12789202]`, you can confirm these are the 3 largest probabilities in `a`. You'll also notice `[3, 0, 5]` are the corresponding indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
